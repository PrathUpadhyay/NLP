{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-3_18210058.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrathUpadhyay/NLP/blob/master/Assignment_3_18210058.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008hvpdW3B6y",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JAPVwHM4026",
        "colab_type": "code",
        "outputId": "9d2fc12a-50ef-4048-bc17-477afb4f8ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "import operator\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCcT4_eG7AeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read file from google-drive, colab doesnt keeps uploaded files persistently\n",
        "\n",
        "tweet_file = open(r\"drive/My Drive/Colab Notebooks/content/nlp assignment 3/train.txt\", encoding=\"utf8\")\n",
        "tweets = tweet_file.read()\n",
        "\n",
        "# test file separate\n",
        "tweets_file_test = open(r\"drive/My Drive/Colab Notebooks/content/nlp assignment 3/test.txt\", encoding=\"utf8\")\n",
        "tweets_test = tweets_file_test.read()\n",
        "\n",
        "# tweets_list = nltk.sent_tokenize(tweets)\n",
        "tweets_list = re.split('\\n\\n',tweets)\n",
        "tweets_list_test = re.split('\\n\\n',tweets_test)\n",
        "\n",
        "# last sentence empty string\n",
        "tweets_list = tweets_list[:-1]\n",
        "tweets_list_test = tweets_list_test[:-1]\n",
        "\n",
        "# tweets_list\n",
        "# len(tweets_list)\n",
        "# sent_tokenize_list = nltk.sent_tokenize(speeches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6shr-3hWf5S",
        "colab_type": "code",
        "outputId": "410dd06f-8edb-4dda-e734-05b87044bd74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#clean train dataset\n",
        "tweets_clean_list = []\n",
        "sentiments_list = []\n",
        "\n",
        "for sent in tweets_list:\n",
        "  # remove \\n\n",
        "  temp = re.sub(\"\\n\", ' ', sent)\n",
        "\n",
        "  # remove ’\n",
        "  temp = re.sub(\"\\’\", '', sent)\n",
        "\n",
        "  # convert to lowercase\n",
        "  temp = temp.lower()\n",
        "\n",
        "  # remove unnecessary punctuations\n",
        "  temp = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in temp]).split())\n",
        "  \n",
        "  # remove first two words from the message, its metadata\n",
        "  temp = temp.split(' ', 3)\n",
        "\n",
        "  # take the next word as sentiment\n",
        "  sentiment = temp[2]\n",
        "\n",
        "  # Retreive only the message\n",
        "  new_msg = temp[3]\n",
        "\n",
        "  new_msg = new_msg.replace(\"hin\", \"\")\n",
        "  new_msg = new_msg.replace(\"eng\", \"\")\n",
        "  new_msg = new_msg.replace(\"o\", \"\")\n",
        "\n",
        "  # emoji removal by emoji pattern\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                        u\"\\U00002702-\\U000027B0\"\n",
        "                        u\"\\U000024C2-\\U0001F251\"\n",
        "                        \"]+\", flags=re.UNICODE)\n",
        "  new_msg = emoji_pattern.sub(r'', new_msg)\n",
        "\n",
        "  # remove multiple consecutive spaces from message\n",
        "  new_msg = re.sub(' +', ' ', new_msg)\n",
        "\n",
        "  #after cleaning up, append to new list\n",
        "  tweets_clean_list.append(new_msg)\n",
        "  sentiments_list.append(sentiment)\n",
        "\n",
        "tweets_clean_list[:10]\n",
        "sentiments_list[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rr6lwWxGFK",
        "colab_type": "code",
        "outputId": "47f5d232-cdef-4eb9-dfb5-becef19ca3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#Clean Test dataset\n",
        "tweets_clean_list_test = []\n",
        "sentiments_list_test = []\n",
        "\n",
        "for sent in tweets_list_test:\n",
        "  # remove \\n\n",
        "  temp = re.sub(\"\\n\", ' ', sent)\n",
        "\n",
        "  # remove ’\n",
        "  temp = re.sub(\"\\’\", '', sent)\n",
        "\n",
        "  # convert to lowercase\n",
        "  temp = temp.lower()\n",
        "\n",
        "  # remove unnecessary punctuations\n",
        "  temp = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in temp]).split())\n",
        "  \n",
        "  # remove first three words from the message, its metadata\n",
        "  temp = temp.split(' ', 3)\n",
        "\n",
        "  # take the next word as sentiment\n",
        "  sentiment = temp[2]\n",
        "\n",
        "  # Retreive only the message\n",
        "  new_msg = temp[3]\n",
        "\n",
        "  new_msg = new_msg.replace(\"hin\", \"\")\n",
        "  new_msg = new_msg.replace(\"eng\", \"\")\n",
        "  new_msg = new_msg.replace(\"o\", \"\")\n",
        "\n",
        "  # emoji removal by emoji pattern\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                        u\"\\U00002702-\\U000027B0\"\n",
        "                        u\"\\U000024C2-\\U0001F251\"\n",
        "                        \"]+\", flags=re.UNICODE)\n",
        "  new_msg = emoji_pattern.sub(r'', new_msg)\n",
        "\n",
        "  # remove multiple consecutive spaces from message\n",
        "  new_msg = re.sub(' +', ' ', new_msg)\n",
        "\n",
        "  #after cleaning up, append to new list\n",
        "  tweets_clean_list_test.append(new_msg)\n",
        "  sentiments_list_test.append(sentiment)\n",
        "\n",
        "\n",
        "tweets_clean_list_test[:10]\n",
        "\n",
        "\n",
        "sentiments_list_test[:10]\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neutral',\n",
              " 'neutral',\n",
              " 'neutral',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCJPQLvLs_M6",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize data using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t1SUxo8tD74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Objects to be used further\n",
        "# tweets_clean_list\n",
        "# sentiments_list\n",
        "# tweets_clean_list_test\n",
        "# sentiments_list_test\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_dict = {\"Message\": tweets_clean_list, \"Sentiment\": sentiments_list}\n",
        "df = pd.DataFrame(train_dict)\n",
        "\n",
        "test_dict = {\"Message\": tweets_clean_list_test, \"Sentiment\": sentiments_list_test}\n",
        "test_df = pd.DataFrame(test_dict)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7)  \n",
        "X = tfidfconverter.fit_transform(df['Message']).toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk38oPgf3qxr",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA4oC7GK3sTr",
        "colab_type": "code",
        "outputId": "55175c5a-73de-4abc-9f82-85ba404a6787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, SimpleRNN, LSTM, TimeDistributed, Embedding\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 300\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(3000, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
        "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 2000, 128)         384000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 300)               514800    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 903       \n",
            "=================================================================\n",
            "Total params: 899,703\n",
            "Trainable params: 899,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcpVlczP9Jf5",
        "colab_type": "code",
        "outputId": "0513b2ac-3b79-4592-976b-04c140288e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "Y = pd.get_dummies(df['Sentiment']).values\n",
        "\n",
        "batch_size = 500\n",
        "\n",
        "model.fit(X, Y, batch_size=batch_size, nb_epoch = 8,  verbose = 1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "15131/15131 [==============================] - 153s 10ms/step - loss: 1.0955 - acc: 0.3694\n",
            "Epoch 2/8\n",
            "15131/15131 [==============================] - 150s 10ms/step - loss: 1.0942 - acc: 0.3700\n",
            "Epoch 3/8\n",
            "15131/15131 [==============================] - 153s 10ms/step - loss: 1.0944 - acc: 0.3725\n",
            "Epoch 4/8\n",
            "15131/15131 [==============================] - 152s 10ms/step - loss: 1.0943 - acc: 0.3726\n",
            "Epoch 5/8\n",
            "15131/15131 [==============================] - 153s 10ms/step - loss: 1.0945 - acc: 0.3726\n",
            "Epoch 6/8\n",
            "15131/15131 [==============================] - 152s 10ms/step - loss: 1.0942 - acc: 0.3726\n",
            "Epoch 7/8\n",
            "15131/15131 [==============================] - 152s 10ms/step - loss: 1.0944 - acc: 0.3726\n",
            "Epoch 8/8\n",
            "15131/15131 [==============================] - 151s 10ms/step - loss: 1.0943 - acc: 0.3726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f776ed81f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XfNVZ_RBoo",
        "colab_type": "text"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDNDfevpwStF",
        "colab_type": "code",
        "outputId": "92ae165d-b6cc-45c7-8935-8aa8fa126fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# X_test, Y_test = tweets_clean_list_test, sentiments_list_test\n",
        "\n",
        "X_test = tfidfconverter.transform(tweets_clean_list_test).toarray()\n",
        "Y_test = pd.get_dummies(test_df['Sentiment']).values\n",
        "\n",
        "\n",
        "_, accuracy = model.evaluate(X_test, Y_test, batch_size = 200, verbose = 1)\n",
        "\n",
        "print(\"Accuracy is \", accuracy)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 10s 5ms/step\n",
            "Accuracy is  0.4034242909690669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uj1Io7EZEV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f23d0f7f-5342-49ab-c6b8-fab68b684838"
      },
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "y_pred1 = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred1, axis=1)\n",
        "Y_test = np.argmax(Y_test, axis = 1 )\n",
        "\n",
        "print (y_pred)\n",
        "# print(len())\n",
        "# print (Y_test[:10])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX8eC_JuQYoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "dcf39dcb-d360-4f85-8dd0-91004e48335e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# print(Y_test)\n",
        "sentiment_names = ['Positive', 'Neutral', 'Negative']\n",
        "print(classification_report(Y_test, y_pred, target_names=sentiment_names))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.00      0.00      0.00       533\n",
            "     Neutral       0.40      1.00      0.57       754\n",
            "    Negative       0.00      0.00      0.00       582\n",
            "\n",
            "    accuracy                           0.40      1869\n",
            "   macro avg       0.13      0.33      0.19      1869\n",
            "weighted avg       0.16      0.40      0.23      1869\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}